# Copiloto Conversacional sobre Documentos
## ‚öôÔ∏è Instrucciones para levantar Docker Compose

## Requisitos en sistema

Antes de levantar los contenedores, aseg√∫rate de tener configurado tu entorno:

- Docker
- Docker Compose
- Una clave de OpenAI o un archivo .env para la prueba
- Puertos libres en tu sistema, para evitar conflictos:
    - 8000 ‚Üí Chroma
    - 3000 ‚Üí Backend
    - 5173 ‚Üí Frontend

## Pasos para levantar el proyecto

### 1. Clonar el repositorio
Ingrasar en linea de consdo en su archivo elegido y clonar el repocitorio con el digiente codigo.

`git clone https://github.com/Pablo-Gajardo/prueba-tecnica-IA.git`

### 2. Entrar a la carpeta  
`cd prueba-tecnica-IA`

### 3. Crear o usar un archivo .env para el backend.
ingresar y crear el archivo .env o ingresar su archivo .env, este achivo deve estar en la direccion correcpondientes al backend.

`cd microservicio-copiloto-pdf`

Si decides crear tu propio archivo .env, debes incluir tu clave de OpenAI de la sigiente manera:

`export OPENAI_API_KEY="tu_clave_aqu√≠"`

### 4. Construir los contenedores.
Ejecuta el siguiente comando para construir todo desde cero (esto instala dependencias y crea im√°genes):

`docker-compose build --no-cache`

### 5. Levantar los contenedores.
Este comando levanta los contenedores que fueron creados previamente.

`docker-compose up`


### 6. Esperar a que el backend se inicie.
Paciencia: el backend puede tardar unos segundos en estar listo. Una vez que est√© corriendo, podr√°s usar la aplicaci√≥n normalmente.

### 7. Acceder al frontend.
Abre tu navegador en la URL: http://localhost:5173


# üìù Descripci√≥n del proyecto

Este proyecto implementa un copiloto conversacional que permite a un usuario subir hasta 5 archivos PDF y hacer 
reguntas en lenguaje natural sobre su contenido. Las respuestas se generan de manera contextual, utilizando un flujo de 
procesamiento y almacenamiento estructurado de la informaci√≥n en estructura Rack.

El objetivo principal es demostrar habilidades en dise√±o de microservicios,
integraci√≥n de LLMs y vector stores, y orquestaci√≥n de flujos conversacionales escalables.

## üè¶ Arquitectura del sistema

Esta es una arquitectura RAG (generaci√≥n aumentada por recuperaci√≥n), basada en las siguientes tecnolog√≠as:

- Backend: NestJS (microservicio)
Elegido por su escalabilidad y patr√≥n de dise√±o modular.
- LLM: GPT-3.5-turbo
Seleccionado por su versatilidad y capacidad de razonamiento l√≥gico.
- Embedding model: text-embedding-3-small
Optimizado por rendimiento y costo para generar vectores de los documentos.
- Vector Store: Chroma
Base de datos vectorial para b√∫squedas sem√°nticas r√°pidas.
- Interfaz: Web app (Vue 3)
Permite subir PDFs y mantener la conversaci√≥n con el asistente.



## üîÑ Flujo conversacional


- Subida de documentos: 
    - El usuario puede subir hasta 5 PDFs.
- Procesamiento de documentos:
    - Cada PDF se divide en chunks (segmentos de texto) para optimizar la b√∫squeda y mejorar la precisi√≥n de los embeddings.
    - Cada chunk se transforma en un vector usando text-embedding-3-small.

- Almacenamiento: 
    - Los vectores generados se guardan en Chroma, permitiendo b√∫squedas sem√°nticas r√°pidas y eficientes.

- Consulta del usuario:
    - Cada pregunta se convierte en un embedding y se busca en Chroma.
    - Los documentos m√°s relevantes se env√≠an al LLM junto con la memoria conversacional y un prompt de orientaci√≥n, garantizando respuestas contextuales y coherentes.

- Generaci√≥n de respuesta:
    - GPT-3.5-turbo produce la respuesta final bas√°ndose en la informaci√≥n recuperada y el contexto de la conversaci√≥n.

## Diagrama de flujo conversacional

![Diagrama de flujo de datos](<img/Diagrama de flujo de datos.png>)

## üöÄ Funcionalidades
Funcionalidades implementadas

- Subida de hasta 5 PDFs.
- Extracci√≥n, chunking y vectorizaci√≥n de contenido.
- B√∫squeda sem√°ntica y recuperaci√≥n de informaci√≥n.
- Interfaz conversacional con memoria contextual.


## üõ†Ô∏è Tecnolog√≠as
Componente	Tecnolog√≠a	Justificaci√≥n
- Backend	NestJS (microservicio):	Escalabilidad, modularidad y patrones de dise√±o claros.
- LLM	GPT-3.5-turbo:	Versatilidad y l√≥gica avanzada en generaci√≥n de texto.
- Embeddings	text-embedding-3-small:	Buen rendimiento y costo-efectivo.
- Vector Store	Chroma:	Recuperaci√≥n sem√°ntica r√°pida y confiable.
- Frontend	Vue 3:	Interfaz reactiva y manejable.


## ‚ö†Ô∏è Limitaciones y mejoras futuras

No se implementa a√∫n la comparaci√≥n autom√°tica de documentos ni clasificaci√≥n avanzada.

### Mejora futura: 
resumir autom√°ticamente documentos largos y a√±adir alertas de inconsistencias.

### Escalabilidad futura: 
implementar pipelines de procesamiento asincr√≥nico para grandes vol√∫menes de PDFs.
